{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sPD8sqQXo3_P"
   },
   "source": [
    "# Estadística para Ciencia de los Datos - Lección 7\n",
    "\n",
    "Autor: Saúl Calderón, Juan Esquivel, Jorge Castro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ok9YXnDIo77r"
   },
   "source": [
    "# Remuestreo\n",
    "\n",
    "Se le denomina remuestreo al proceso de tomar múltiples muestras sobre una misma población base. \n",
    "Los métodos estadísticos de remuestreo nos permiten  estimar parámetros poblacionales y **cuantificar la incertidumbre del estimado** (similar a los intervalos de confianza). Además se pueden usar para **evaluar y mejorar la exactitud de algunos modelos de aprendizaje automático**.\n",
    "\n",
    "\n",
    "En general, los métodos de remuestreo son fáciles de usar y requieren poco conocimiento matemático en comparación a otros métodos de estadística inferencial. El remuestreo no se realiza para proveer un estimado sobre la distribución poblacional, sino más bien para proveer un estimado de la distribución muestral del estadístico de interés. A continuación veremos dos métodos de remuestreo: Bootstrap y Empaquetamiento (*bagging*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lEzRh_1hp2Lz"
   },
   "source": [
    "## Bootstrap\n",
    "\n",
    "Una manera efectiva y sencilla de estimar la distribución muestral de un estadístico, consiste en extraer muestras adicionales (bootstrap samples) con reemplazo de la misma muestra y recalcular el estadístico para cada remuestreo. A este procedimiento se le conoce como *Bootstrap* y no requiere necesariamente de ninguna suposición sobre la normalidad de los datos o el estadístico de interés.\n",
    "\n",
    "El algoritmo de bootstrap para una muestra de tamaño $n$ con $R$ iteraciones se detalla a continuación:\n",
    "\n",
    "1. Tomar una observación y guardarla con reemplazo (es decir, no se elimina la observación tomada del conjunto de datos).\n",
    "2.  Repetir $n$ veces.\n",
    "3. Calcular la media de los $n$ valores remuestreados.\n",
    "4. Repetir los pasos del 1 al 3 $R$ veces.\n",
    "5. Usar los $R$ resultados para:\n",
    "  - Calcular su desviación estándar (que estima la desviación estándar de la media muestral $\\sigma_{\\overline{x}}$).\n",
    "  - Producir un histograma o *boxplot*.\n",
    "  - Encontrar un intervalo de confianza.\n",
    "\n",
    "Entre más iteraciones $R$ se ejecuten más preciso será el estimado de $\\sigma_{\\overline{x}}$ y el intervalo de confianza. El resultado del algoritmo de *Bootstrap* es un conjunto de estadísticas muestrales o parámetros estimados del modelo que se pueden examinar para ver que tan variables son. En la práctica un modelo se puede ejecutar sobre los datos producidos con el Bootstrap para estimar la estabilidad de los parámetros del modelo o para mejorar su poder predictivo.\n",
    "\n",
    "Es importante entender que la técnica de *bootstrap* no es una compensación para tamaños de muestra pequeños, tampoco crea datos nuevos ni rellena hoyos en un conjunto de datos existente. Tan solo informa sobre como una gran cantidad de muestras adicionales se comportarían si se toman de una población como la de nuestra muestra original. Una desventaja del bootstrap es que puede ser computacionalmente costoso.\n",
    "\n",
    "Finalmente, sobre el Bootstrap cabe resaltar que:\n",
    "\n",
    "1. Permite evaluar la variabilidad de un estadístico muestral.\n",
    "2. Permite estimar las distribuciones muestrales de estadísticos para los cuales no se ha desarrollado una aproximación matemática.\n",
    "3. Cuando se aplica modelos predictivos, la agregación de múltiples predicciones muestrales basadas en bootstrap (bagging) suele ser superior al uso de un único modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXeTqXVxrW3b"
   },
   "source": [
    "## Empaquetado o bagging\n",
    "\n",
    "*Bagging* se refiere a un meta-algoritmo de aprendizaje por ensambles\n",
    "o *consejos*, diseñado para mejorar la estabilidad de las predicciones en modelos de regresión o clasificación.\n",
    "\n",
    "Sea un conjunto de observaciones de entrenamiento:\n",
    "\n",
    "\\begin{equation}\n",
    "X=\\left\\{ \\overrightarrow{x}_{1},\\overrightarrow{x}_{2},\\ldots,\\overrightarrow{x}_{k}\\right\\} ,\n",
    "\\end{equation}\n",
    "\n",
    " el empaquetamiento genera entonces $m$ nuevos conjuntos de datos $\\left\\{ X_{1},\\ldots,X_{m}\\right\\} $, con cada conjunto de datos compuesto por $k$ observaciones, muestreadas del conjunto de datos original $X$ **con reemplazo**. Para cada conjunto de datos $X_{i}$ (conocido como **muestra bootstrap**) el método de empaquetado propone crear y entrenar un modelo $c_{i}\\left(X_i\\right)$. Finalmente, \n",
    "los resultados de cada modelo son **promediados (para el caso de la regresión)**\n",
    "o utilizados para realizar una **votación (en el caso de la\n",
    "clasificación)**.\n",
    "\n",
    "El empaquetado es recomendado para casos donde los modelos presentan\n",
    "una variabilidad alta en sus salidas, por lo que puede estabilizar\n",
    "las predicciones, sin embargo, en casos donde la variabilidad de la\n",
    "predicción en el modelo sea baja, el empaquetado puede degradar la\n",
    "exactitud de las predicciones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8qvE3y4sx4gb"
   },
   "source": [
    "# Ejemplo diabetes\n",
    "\n",
    "(Tomado de https://www.kaggle.com/kumargh/pimaindiansdiabetescsv):\n",
    "\n",
    "Predicción sobre desarrollo de diabetes en pacientes (o no) en los próximos 5 años. Descripción de los atributos extraída de kaggle:\n",
    "\n",
    " $x_0$:  Number of times pregnant.\n",
    " \n",
    " $x_1:$ Plasma glucose concentration a 2 hours in an oral glucose tolerance test.\n",
    " \n",
    " $x_2:$ Diastolic blood pressure (mm Hg).\n",
    " \n",
    " $x_3:$ Triceps skinfold thickness (mm).\n",
    " \n",
    " $x_4:$ 2-Hour serum insulin (mu U/ml).\n",
    " \n",
    " $x_5:$ Body mass index (weight in kg/(height in m)^2).\n",
    " \n",
    " $x_6:$ Diabetes pedigree function.\n",
    " \n",
    " $x_7: $Age (years).\n",
    "\n",
    "La columna número 8 representa el objetivo de predicción $t$ (desarrollo de diabetes o no). Por lo tanto, cada observación está dado por $\\vec{x} \\in \\mathbb{R}^8$ y el problema a abordar es una clasificación binaria (desarrollar diabetes o no).\n",
    "\n",
    "El conjunto de datos tiene $m=768$ observaciones o muestras.\n",
    "\n",
    "Un modelo lineal base da una certeza de 65% de predicción, mientras que los mejores resultados rondan 77%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "zhDpN5rWs-qT",
    "outputId": "5eee838b-68f6-4604-c804-e34921959d71"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 739
    },
    "colab_type": "code",
    "id": "zc-OPvau1bEf",
    "outputId": "d095c602-364a-49ce-ac60-6c969cc8c416"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0           1           2           3           4           5  \\\n",
      "count  768.000000  768.000000  768.000000  768.000000  768.000000  768.000000   \n",
      "mean     3.845052  120.894531   69.105469   20.536458   79.799479   31.992578   \n",
      "std      3.369578   31.972618   19.355807   15.952218  115.244002    7.884160   \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "25%      1.000000   99.000000   62.000000    0.000000    0.000000   27.300000   \n",
      "50%      3.000000  117.000000   72.000000   23.000000   30.500000   32.000000   \n",
      "75%      6.000000  140.250000   80.000000   32.000000  127.250000   36.600000   \n",
      "max     17.000000  199.000000  122.000000   99.000000  846.000000   67.100000   \n",
      "\n",
      "                6           7           8  \n",
      "count  768.000000  768.000000  768.000000  \n",
      "mean     0.471876   33.240885    0.348958  \n",
      "std      0.331329   11.760232    0.476951  \n",
      "min      0.078000   21.000000    0.000000  \n",
      "25%      0.243750   24.000000    0.000000  \n",
      "50%      0.372500   29.000000    0.000000  \n",
      "75%      0.626250   41.000000    1.000000  \n",
      "max      2.420000   81.000000    1.000000  \n",
      "     0    1   2   3    4     5      6   7  8\n",
      "0    6  148  72  35    0  33.6  0.627  50  1\n",
      "1    1   85  66  29    0  26.6  0.351  31  0\n",
      "2    8  183  64   0    0  23.3  0.672  32  1\n",
      "3    1   89  66  23   94  28.1  0.167  21  0\n",
      "4    0  137  40  35  168  43.1  2.288  33  1\n",
      "5    5  116  74   0    0  25.6  0.201  30  0\n",
      "6    3   78  50  32   88  31.0  0.248  26  1\n",
      "7   10  115   0   0    0  35.3  0.134  29  0\n",
      "8    2  197  70  45  543  30.5  0.158  53  1\n",
      "9    8  125  96   0    0   0.0  0.232  54  1\n",
      "10   4  110  92   0    0  37.6  0.191  30  0\n",
      "11  10  168  74   0    0  38.0  0.537  34  1\n",
      "12  10  139  80   0    0  27.1  1.441  57  0\n",
      "13   1  189  60  23  846  30.1  0.398  59  1\n",
      "14   5  166  72  19  175  25.8  0.587  51  1\n",
      "15   7  100   0   0    0  30.0  0.484  32  1\n",
      "16   0  118  84  47  230  45.8  0.551  31  1\n",
      "17   7  107  74   0    0  29.6  0.254  31  1\n",
      "18   1  103  30  38   83  43.3  0.183  33  0\n",
      "19   1  115  70  30   96  34.6  0.529  32  1\n",
      "number of zeros per column\n",
      "0    111\n",
      "1      5\n",
      "2     35\n",
      "3    227\n",
      "4    374\n",
      "5     11\n",
      "6      0\n",
      "7      0\n",
      "8    500\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "#read the dataset from csv\n",
    "dataset = read_csv('pima-indians-diabetes.csv', header=None)\n",
    "\n",
    "#print descriptive stats\n",
    "print(dataset.describe())\n",
    "\n",
    "# print the first 20 rows of data\n",
    "print(dataset.head(20))\n",
    "\n",
    "print(\"number of zeros per column\")\n",
    "print((dataset == 0).astype(int).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MgvCjrjR2wUd"
   },
   "source": [
    "## Marcar datos nulos\n",
    "\n",
    "En algunas columnas como \"Triceps skinfold thickness\" o \"diastolic pressure\", los valores de cero no tienen sentido. \n",
    "Una posible estrategia para trabajar con estos valores inválidos o faltantes consiste en eliminar todas las observaciones (filas) que contengan al menos una columna con algún valor faltante.\n",
    "\n",
    "\n",
    "Para las columnas del 1-5 los valores 0 primero serán reemplazados por `NaN` usando la función `replace`. Luego se borran todas las filas con valores `NaN`, lo cuál incrementa el riesgo de *over fitting*. (ejercicio posterior: usar al menos 2 técnicas más de imputación y aplicar la técnica de bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "colab_type": "code",
    "id": "Yxp1vmUU4VEZ",
    "outputId": "df21f3f3-9e5c-4983-be55-bbd5008971f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null observations per column: \n",
      "0      0\n",
      "1      5\n",
      "2     35\n",
      "3    227\n",
      "4    374\n",
      "5     11\n",
      "6      0\n",
      "7      0\n",
      "8      0\n",
      "dtype: int64\n",
      "Number of null observations per column after NaN entry deletion: \n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "5    0\n",
      "6    0\n",
      "7    0\n",
      "8    0\n",
      "dtype: int64\n",
      "                0           1           2           3           4           5  \\\n",
      "count  392.000000  392.000000  392.000000  392.000000  392.000000  392.000000   \n",
      "mean     3.301020  122.627551   70.663265   29.145408  156.056122   33.086224   \n",
      "std      3.211424   30.860781   12.496092   10.516424  118.841690    7.027659   \n",
      "min      0.000000   56.000000   24.000000    7.000000   14.000000   18.200000   \n",
      "25%      1.000000   99.000000   62.000000   21.000000   76.750000   28.400000   \n",
      "50%      2.000000  119.000000   70.000000   29.000000  125.500000   33.200000   \n",
      "75%      5.000000  143.000000   78.000000   37.000000  190.000000   37.100000   \n",
      "max     17.000000  198.000000  110.000000   63.000000  846.000000   67.100000   \n",
      "\n",
      "                6           7           8  \n",
      "count  392.000000  392.000000  392.000000  \n",
      "mean     0.523046   30.864796    0.331633  \n",
      "std      0.345488   10.200777    0.471401  \n",
      "min      0.085000   21.000000    0.000000  \n",
      "25%      0.269750   23.000000    0.000000  \n",
      "50%      0.449500   27.000000    0.000000  \n",
      "75%      0.687000   36.000000    1.000000  \n",
      "max      2.420000   81.000000    1.000000  \n"
     ]
    }
   ],
   "source": [
    "dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, np.NaN)\n",
    "print(\"Number of null observations per column: \")\n",
    "print(dataset.isnull().sum())\n",
    "\n",
    "#we can eliminate the observations or samples with nans\n",
    "dataset = dataset.dropna()\n",
    "print(\"Number of null observations per column after NaN entry deletion: \")\n",
    "print(dataset.isnull().sum())\n",
    "\n",
    "#print descriptive stats\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wXmxHebr4v81"
   },
   "source": [
    "## Bagging / Ensemble Learning\n",
    "\n",
    "Código base para aplicar *Bagging* (aprendizaje por ensambles\n",
    "o *consejos*) para clasificación, con votación para la decisión final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "4LPuzOe-5Y0v",
    "outputId": "972a76ec-c698-4b68-8033-075f32a3f972"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1  0.76  D2  0.75  D3  0.73\n",
      "Ensemble  0.79\n"
     ]
    }
   ],
   "source": [
    "# split dataset into inputs and outputs\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy import stats\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Model test function\n",
    "def testModel(dataset):\n",
    "  values = dataset.values\n",
    "  \n",
    "  \n",
    "  Xtraining = values[:292,0:8]\n",
    "  Ttraining = values[:292,8]\n",
    "  \n",
    "  Xtest = values[292:,0:8]\n",
    "  Ttest = values[292:,8]\n",
    "\n",
    "  #use K-nearest Neighbors Classifier and two Decision Trees\n",
    "  model1 = KNeighborsClassifier() \n",
    "  model2 = tree.DecisionTreeClassifier(criterion='entropy') \n",
    "  model3 = tree.DecisionTreeClassifier(criterion='gini')\n",
    "  \n",
    "  #fit models\n",
    "  model1.fit(Xtraining, Ttraining)\n",
    "  model2.fit(Xtraining, Ttraining) \n",
    "  model3.fit(Xtraining, Ttraining) \n",
    "  \n",
    "  #make final predictions\n",
    "  predictions1 = model1.predict(Xtest)\n",
    "  predictions2 = model2.predict(Xtest)\n",
    "  predictions3 = model3.predict(Xtest)\n",
    "  \n",
    "   \n",
    "  finalPredictions = np.array([])\n",
    "  \n",
    "  for i in range(0, len(Ttest)):\n",
    "    mode = stats.mode([predictions1[i], predictions2[i], predictions3[i]])    \n",
    "    #print(\"Mode \", mode, \" p1 \", predictions1[i],  \" p2 \", predictions2[i], \" p3 \", predictions3[i])   \n",
    "    #Prediction of the ensemble\n",
    "    finalPredictions = np.append(finalPredictions, mode[0]) \n",
    "\n",
    "  #Evaluation metric\n",
    "  dist1 = accuracy_score(Ttest, predictions1) \n",
    "  dist2 = accuracy_score(Ttest, predictions2) \n",
    "  dist3 = accuracy_score(Ttest, predictions3) \n",
    "  distEnsemble = accuracy_score(Ttest, finalPredictions)\n",
    "  \n",
    "  print(\"D1 \", dist1, \" D2 \", dist2, \" D3 \", dist3)\n",
    "  print(\"Ensemble \", distEnsemble)\n",
    "\n",
    "  \n",
    "#Function call  \n",
    "testModel(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IkdJve_oH7gf"
   },
   "source": [
    "## Ejercicio:\n",
    "\n",
    "Haga las correcciones pertinentes para implementar correctamente la técnica de empaquetado o *bagging* de acuerdo al material visto en esta lección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.     89.     66.    ...  28.1     0.167  21.   ]\n",
      " [  0.    137.     40.    ...  43.1     2.288  33.   ]\n",
      " [  3.     78.     50.    ...  31.      0.248  26.   ]\n",
      " ...\n",
      " [  1.     87.     68.    ...  37.6     0.401  24.   ]\n",
      " [  6.     99.     60.    ...  26.9     0.497  32.   ]\n",
      " [  2.     95.     54.    ...  26.1     0.748  22.   ]]\n",
      "     8\n",
      "3    0\n",
      "4    1\n",
      "6    1\n",
      "8    1\n",
      "13   1\n",
      "..  ..\n",
      "753  1\n",
      "755  1\n",
      "760  0\n",
      "763  0\n",
      "765  0\n",
      "\n",
      "[392 rows x 1 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [292, 392]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [53]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m labels \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39miloc[:,[\u001b[38;5;241m8\u001b[39m]]\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(labels)\n\u001b[1;32m---> 61\u001b[0m \u001b[43mtestModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [53]\u001b[0m, in \u001b[0;36mtestModel\u001b[1;34m(features, labels)\u001b[0m\n\u001b[0;32m     12\u001b[0m values \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Se sustituye esta parte\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Xtraining = values[:292,0:8]\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Ttraining = values[:292,8]\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Xtest = values[292:,0:8]\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Ttest = values[292:,8]\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m Xtraining, Xtest, Ttraining, Ttest \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#use K-nearest Neighbors Classifier and two Decision Trees\u001b[39;00m\n\u001b[0;32m     24\u001b[0m model1 \u001b[38;5;241m=\u001b[39m KNeighborsClassifier() \n",
      "File \u001b[1;32mC:\\Alberto\\Ciencia de Datos TEC\\jupyter\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2417\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2417\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2419\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   2420\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2421\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2422\u001b[0m )\n",
      "File \u001b[1;32mC:\\Alberto\\Ciencia de Datos TEC\\jupyter\\lib\\site-packages\\sklearn\\utils\\validation.py:378\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    377\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 378\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mC:\\Alberto\\Ciencia de Datos TEC\\jupyter\\lib\\site-packages\\sklearn\\utils\\validation.py:332\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    330\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    335\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [292, 392]"
     ]
    }
   ],
   "source": [
    "# split dataset into inputs and outputs\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy import stats\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Model test function\n",
    "def testModel(features, labels):\n",
    "    values = dataset.values\n",
    "  \n",
    "    # Se sustituye esta parte\n",
    "    # Xtraining = values[:292,0:8]\n",
    "    # Ttraining = values[:292,8]\n",
    "    # Xtest = values[292:,0:8]\n",
    "    # Ttest = values[292:,8]\n",
    "\n",
    "\n",
    "    Xtraining, Xtest, Ttraining, Ttest = train_test_split(features, labels, test_size=0.30, random_state=42)\n",
    "\n",
    "    #use K-nearest Neighbors Classifier and two Decision Trees\n",
    "    model1 = KNeighborsClassifier() \n",
    "    model2 = tree.DecisionTreeClassifier(criterion='entropy') \n",
    "    model3 = tree.DecisionTreeClassifier(criterion='gini')\n",
    "\n",
    "    #fit models\n",
    "    model1.fit(Xtraining, Ttraining)\n",
    "    model2.fit(Xtraining, Ttraining) \n",
    "    model3.fit(Xtraining, Ttraining) \n",
    "\n",
    "    #make final predictions\n",
    "    predictions1 = model1.predict(Xtest)\n",
    "    predictions2 = model2.predict(Xtest)\n",
    "    predictions3 = model3.predict(Xtest)\n",
    "\n",
    "\n",
    "    finalPredictions = np.array([])\n",
    "\n",
    "    for i in range(0, len(Ttest)):\n",
    "        mode = stats.mode([predictions1[i], predictions2[i], predictions3[i]])    \n",
    "        #print(\"Mode \", mode, \" p1 \", predictions1[i],  \" p2 \", predictions2[i], \" p3 \", predictions3[i])   \n",
    "        #Prediction of the ensemble\n",
    "        finalPredictions = np.append(finalPredictions, mode[0]) \n",
    "\n",
    "    #Evaluation metric\n",
    "    dist1 = accuracy_score(Ttest, predictions1) \n",
    "    dist2 = accuracy_score(Ttest, predictions2) \n",
    "    dist3 = accuracy_score(Ttest, predictions3) \n",
    "    distEnsemble = accuracy_score(Ttest, finalPredictions)\n",
    "\n",
    "    print(\"D1 \", dist1, \" D2 \", dist2, \" D3 \", dist3)\n",
    "    print(\"Ensemble \", distEnsemble)\n",
    "  \n",
    "# Function call\n",
    "features = dataset.values[:292,0:8]\n",
    "print(features)\n",
    "labels = dataset.iloc[:,[8]]\n",
    "print(labels)\n",
    "testModel(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.     89.     66.    ...  28.1     0.167  21.   ]\n",
      " [  0.    137.     40.    ...  43.1     2.288  33.   ]\n",
      " [  3.     78.     50.    ...  31.      0.248  26.   ]\n",
      " ...\n",
      " [  1.     87.     68.    ...  37.6     0.401  24.   ]\n",
      " [  6.     99.     60.    ...  26.9     0.497  32.   ]\n",
      " [  2.     95.     54.    ...  26.1     0.748  22.   ]]\n"
     ]
    }
   ],
   "source": [
    "features = dataset.values[:292,0:8]\n",
    "print(features)\n",
    "\n",
    "\n",
    "#labels = dataset.iloc[:,[8]]\n",
    "#print(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RIGKZofM0b5L"
   },
   "source": [
    "# Práctica para Exámen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RQbnO-pl0frN"
   },
   "source": [
    "1. Si se tienen 2 eventos $A$ y $B$ con probabilidades $p_1$ y $p_2$ respectivamente, ¿Cuál es la probabilidad máxima de ocurrencia de ambos eventos en términos de sus probabilidades individuales $p_1$ y $p_2$?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zymZecYq0g0q"
   },
   "source": [
    "2. Explique cuál es la diferencia entre la probabilidad conjunta de 2 variables aleatorias $X$ y $Y$ y la probabilidad condicional de las mismas. Matemáticamente, ¿Cómo se encuentran relacionadas estas probabilidades?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "25_UsdYC0hXX"
   },
   "source": [
    "3. El 40% de las ventas en una tienda de ropa corresponden a artículos con descuento. Si los clientes devuelven el 15% de los artículos que compran con descuento y el 6% de los artículos que compran sin descuento ¿Cuál es el porcentaje global de artículos devueltos? Además, ¿Cuál es la probabilidad de que un cliente haya comprado un artículo con descuento dado que no fue devuelto?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IZlKyzpW0iLa"
   },
   "source": [
    "4. ¿Cómo se calcula el rango intercuartil en los diagramas de cajas y que permite determinar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7JeF8uU0kAs"
   },
   "source": [
    "5. Mediante un muestreo aleatorio se han recabado los siguientes tiempos de realización de exámenes del curso de estadística:\n",
    "\n",
    "$90.5 \\quad 187.2 \\quad 54.7 \\quad 127.4 \\quad 182.1 \\quad 260.5 \\quad 140.3 \\quad 209.2 \\quad 277.9 \\quad 10.7$\n",
    "\n",
    "Obtenga la estimación de máxima verosimilitud del tiempo medio de realización de la prueba y su desviación estándar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sn43Y7pc_u3e"
   },
   "source": [
    "6. ¿Cuál es la diferencia entre un estimador puntual y un intervalo de confianza?¿Qué significa el nivel de confianza y como se expresa matemáticamente?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OTskiOl__ylP"
   },
   "source": [
    "7. Suponga que la probabilidad de mal funcionamiento de un motor durante cualquier periodo de una hora es p = 0.02. Encuentre la probabilidad de que un motor determinado funcione bien \n",
    "al menos tres horas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y9bmYP6i_x9V"
   },
   "source": [
    "8. ¿Cuál es la diferencia entre la distribución t-student y la distribución Z? Según el contexto del curso ¿En qué casos conviene usar una o la otra?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LqgcJm0pi0QK"
   },
   "source": [
    "9. El vicepresidente de ventas de una gran compañía afirma que los vendedores están promediando no más de 15 contactos de venta por semana. Como prueba de su afirmación, aleatoriamente se seleccionan $n=16$ vendedores y se registra el número de contactos hechos por cada uno para una sola semana seleccionada al azar. La media y varianza de las 16 mediciones fueron 17 y 9, respectivamente. ¿La evidencia contradice lo dicho por el vicepresidente? Use una prueba con nivel de significancia $\\alpha=0.05$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "3TdOqgbZn7oV",
    "outputId": "c714a41c-a97f-45cb-8b4b-1d58e5cbf9a9"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm, t\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t2Dv97rTuSz1"
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 7, saw 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.kaggle.com/kumargh/pimaindiansdiabetescsv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m df\n",
      "File \u001b[1;32mC:\\Alberto\\Ciencia de Datos TEC\\jupyter\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Alberto\\Ciencia de Datos TEC\\jupyter\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Alberto\\Ciencia de Datos TEC\\jupyter\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:581\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Alberto\\Ciencia de Datos TEC\\jupyter\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1250\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1248\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1250\u001b[0m     index, columns, col_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mC:\\Alberto\\Ciencia de Datos TEC\\jupyter\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:225\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 225\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    227\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mC:\\Alberto\\Ciencia de Datos TEC\\jupyter\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:805\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\Alberto\\Ciencia de Datos TEC\\jupyter\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\Alberto\\Ciencia de Datos TEC\\jupyter\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:847\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\Alberto\\Ciencia de Datos TEC\\jupyter\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1960\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 7, saw 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.kaggle.com/kumargh/pimaindiansdiabetescsv\"\n",
    "df = pd.read_csv(url)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Leccion7_EstadisticaParaCienciaDeLosDatos.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
